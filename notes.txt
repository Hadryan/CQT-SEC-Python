Plan:
- Get tiny subset from NSynth dataset to test PILSE and compare to MFCCs.
- If works, read all the papers to see what people did
- Get smaller subset from NSynth for the dataset
- Find a way to compare everything with each other


Papers:

- An Attention Mechanism for Musical Instrument Recognition - 2019
https://arxiv.org/pdf/1907.04294.pdf
OpenMIC, VGGish, no link

- Openmic-2018: An open dataset for multiple instrument recognition - 2018
https://brianmcfee.net/papers/ismir2018_openmic.pdf


https://mac.kaist.ac.kr/pubs/HanLeeNamLee-jasa2016.pdf
https://arxiv.org/pdf/1808.09730.pdf
https://arxiv.org/pdf/1605.06644.pdf

Perfecto Herrera-Boyer, Geoffroy Peeters, and Shlomo
Dubnov. Automatic classification of musical instrument
sounds. Journal of New Music Research, 32(1):3–21,
2003.

Tetsuro Kitahara, Masataka Goto, Kazunori Komatani,
Tetsuya Ogata, and Hiroshi G Okuno. Instrument identification in polyphonic music: Feature weighting to
minimize influence of sound overlaps. EURASIP Journal on Applied Signal Processing, 2007(1):155–155,
2007.

Yoonchang Han, Jaehun Kim, and Kyogu Lee. Deep
convolutional neural networks for predominant instrument recognition in polyphonic music. IEEE/ACM
Transactions on Audio, Speech and Language Processing (TASLP), 25(1):208–221, 2017.


Datasets:
- IRMAS: weakly-labeled (no)
- MedleyDB: multi-track (ok)
- Mixing Secrets: multi-track (ok)
- OpenMIC: weakly-labeled (no)
- RWC (2003)


NSynth dataset:
305,979 musical notes, each with a unique pitch, timbre, and envelope. For 1,006 instruments from commercial sample libraries, we generated four second, monophonic 16kHz audio snippets, referred to as notes, by ranging over every pitch of a standard MIDI piano (21-108) as well as five different velocities (25, 50, 75, 100, 127). The note was held for the first three seconds and allowed to decay for the final second.